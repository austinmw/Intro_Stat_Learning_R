library(MASS)
library(ISLR)
# 3.6.2  Simple Linear Regression
# ===============================
fix(Boston)
names(Boston)
# fit a regression model with syntax of the form lm(y~x,data)
attach(Boston)
lm.fit=lm(medv~lstat)
# basic info about the model
lm.fit
# more detailed info
summary(lm.fit)
# see what info is stored in lm.fit
names(lm.fit)
# extractor function coef as safe way to access
coef(lm.fit)
# confidence einterval
confint(lm.fit)
# confidence intervals and prediction intervals for prediction of medv for a given value of lstat
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval="confidence")
predict(lm.fit, data.frame(lstat=c(5,10,15)), interval="prediction")
# e.g. the 95% confidence interval associated with a lstat value of 10 is (24.47,25.63),
# and the 95% prediction interval is (12.828, 37.28)
# remember,
# Confidence intervals tell you about how well you have determined the mean.
# Prediction intervals tell you where you can expect to see the next data point sampled.
# same center, but prediction intervals are substantially wider
# plot medv and lstat along with the least squares regression line
plot(lstat,medv)
abline(lm.fit) # (intercept a, slope b)
# notice potential nonlinearity
# some additional settings for plotting lines and points
abline(lm.fit, lwd=5)
abline(lm.fit, lwd=5, col="red")
plot(lstat,medv,col="red")
plot(lstat,medv,pch=20)
plot(lstat,medv,pch="+")
plot(1:20,1:20,pch=1:20)
# examining some diagnostic plots
# divide plotting region
par(mfrow=c(2,2))
plot(lm.fit)
# by default, plot() on lm.fit automatically produces these four diagnostic plots
# alternatively compute residuals from linear regression fit
# rstudent() will return the studentized residuals
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
# on the basis of the residual plots, there is some evidence of non-linearity
# leverage statistics can be computed with hatvalues()
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
# which.max() identifies the index of the largest element of a vector
# in this case, it tells which observation has the largest leverage statistic
# 3.6.3  Multiple Linear Regression
# =================================
# multiple linear regression syntax is lm(y~x1+x2+x3)
lm.fit=lm(medv~lstat+age,data=Boston)
summary(lm.fit)
# short-hand for all variables
lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)
# see what's available
?summary.lm
# access individual components of a summary object
summary(lm.fit)$r.sq # R^2
summary(lm.fit)$sigma # RSE
# variance inflation factors (VIF's) using the car package
library(car)
vif(lm.fit)
# VIF's are low to moderate for this data
# perform regression using all variables except one
lm.fit=lm(medv~.-age,data=Boston)
summary(lm.fit)
# alternatively, the update function can be used
lm.fit1=update(lm.fit, ~.-age)
# 3.6.4  Interaction Terms
# ========================
# syntax to include interaction term between lstat and black: lstat:black
# syntax to simultaneously include lstat,age, and lstat*age as predictors: lstat*age
summary(lm(medv~lstat*age,data=Boston))
# 3.6.5  Non-linear Transformations of the Predictors
# ===================================================
# given predictor X, can create predictor X^2 using I(X^2)
# perform regression of medv onto lstat and lstat^2
lm.fit2=lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
# near-zero p-value associated with quadratic term suggests that it leads to an improved model
# use anova() to further quantify the extent to which the quadratic fit is superior to the linear fit
lm.fit=lm(medv~lstat)
anova(lm.fit,lm.fit2)
# anova function performs hypothesis test comparing the two models
# the null hypothesis is that the two models fit the data equally well
# the alternative hypothesis is that the full model is superior
# F-statistic of 135 and associated p-value is virtually zero provides clear evidence that the full model is superior
# not suprising, since earlier we saw evidence for non-linearity in relationshio ebtween medv and lstat
par(mfrow=c(2,2))
plot(lm.fit2)
# when lstat^2 term is included in the model, there is alittle discernible pattern in the residuals (good)
# to include a cubic fit, we can add a predictor of the form I(X^3)
# poly() instead can create the polynomial within lm()
# producing a 5th order polynomial fit
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)
# this suggests polynomial terms up to the fifth order leads to an improvement in the model fit
# taking a look at the 6th term, we'd notice that terms beyond the fifth order have significant p-values
# we can also do log transformations instead
summary(lm(medv~log(rm),data=Boston))
# 3.6.6  Qualitative Predictors
# =============================
# Taking a look at Carseats dat which is part of ISLR
fix(Carseats)
names(Carseats)
# Carseats data includes qualitative predictors such as Shelveloc
# given a qualitative variable, R generates dummy variables automatically
# below we fit a multiple regression model that includes some interaction terms
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
# the contrasts() function returns coding that R uses for the dummy variables
attach(Carseats)
contrasts(ShelveLoc)
# R created ShelveLocGood and ShelveLocMedium dummy variables
# notice that ShelveLocGood has a positive, larger regression coefficient,
# while ShelveLocMedium has a smaller positive coefficient,
# indicating that a medium shelving location leads to higher sales than a bad shelving location,
# but lower sales than a good shelving location.
# 3.6.7  Writing Functions
# ========================
# create a simple function that reads in the ISLR and MASS libraries
LoadLibraries=function(){
library(ISLR)
library(MASS)
print("The libraries have been loaded.")
}
# print the function
LoadLibraries
# call the function
LoadLibraries()
